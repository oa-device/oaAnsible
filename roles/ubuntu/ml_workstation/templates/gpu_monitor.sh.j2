#!/bin/bash
# Ubuntu GPU Monitoring Script for NVIDIA GPUs
# Generated by Ansible oaAnsible/roles/ubuntu/ml_workstation

echo "Ubuntu GPU Monitor"
echo "===================="

# Check if NVIDIA drivers are loaded
if ! command -v nvidia-smi >/dev/null 2>&1; then
    echo "[FAIL] NVIDIA drivers not installed or not in PATH"
    echo "Run 'sudo apt install nvidia-driver-<version>' to install drivers"
    exit 1
fi

# GPU information
echo ""
echo "GPU Information:"
nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu,utilization.memory,temperature.gpu,power.draw --format=csv,noheader,nounits

# Real-time monitoring function
monitor_gpu() {
    echo ""
    echo "Real-time GPU Monitoring (Press Ctrl+C to stop):"
    echo "================================================"
    
    while true; do
        clear
        echo "GPU Status - $(date)"
        echo "========================================"
        
        # GPU utilization and memory
        nvidia-smi --query-gpu=index,name,utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu,power.draw --format=csv,noheader,nounits | while IFS=',' read -r idx name gpu_util mem_util mem_used mem_total temp power; do
            echo "GPU $idx: $name"
            echo "  Utilization: ${gpu_util}% GPU, ${mem_util}% Memory"
            echo "  Memory: ${mem_used}MB / ${mem_total}MB"
            echo "  Temperature: ${temp}Â°C"
            echo "  Power: ${power}W"
            echo ""
        done
        
        # Process information
        echo "Running GPU Processes:"
        nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv,noheader,nounits | head -5
        
        # System load
        echo ""
        echo "System Load:"
        echo "  CPU: $(uptime | awk -F'load average:' '{print $2}')"
        echo "  Memory: $(free -h | awk '/^Mem:/ {printf "%.1fGB/%.1fGB (%.1f%%)", $3/1024, $2/1024, $3/$2*100}')"
        
        sleep 2
    done
}

# Training monitoring function  
monitor_training() {
    echo ""
    echo "ML Training Monitor:"
    echo "==================="
    
    # Check for common ML processes
    echo "Active ML Processes:"
    ps aux | grep -E "(python.*train|python.*yolo|jupyter)" | grep -v grep | head -5
    
    echo ""
    echo "GPU Memory Timeline (last 10 readings):"
    for i in {1..10}; do
        mem_used=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)
        mem_total=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits)
        gpu_util=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)
        echo "$(date '+%H:%M:%S'): ${mem_used}/${mem_total}MB (${gpu_util}% util)"
        sleep 1
    done
}

# Main menu
case "${1:-info}" in
    "monitor"|"watch")
        monitor_gpu
        ;;
    "training")
        monitor_training
        ;;
    "info"|*)
        echo ""
        echo "System Summary:"
        echo "=================="
        echo "Hostname: $(hostname)"
        echo "Uptime: $(uptime -p)"
        echo "CUDA Version: $(nvcc --version 2>/dev/null | grep 'release' | awk '{print $6}' || echo 'Not found')"
        echo ""
        echo "Available commands:"
        echo "  gpu_monitor info     - Show this information"
        echo "  gpu_monitor monitor  - Real-time GPU monitoring"
        echo "  gpu_monitor training - ML training specific monitoring"
        echo ""
        echo "Quick aliases:"
        echo "  gpu-watch  - Real-time monitoring"
        echo "  gpu-top    - GPU process list"
        ;;
esac