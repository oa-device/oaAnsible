#!/bin/bash
# oaSentinel Automated Training Script
# Generated by Ansible for {{ inventory_hostname }}
# Optimized for {{ ansible_processor_vcpus | default(4) }} CPU cores, {{ ((ansible_memtotal_mb | default(8192))/1024)|round|int }}GB RAM
{% if ml_has_nvidia_gpu is defined and ml_has_nvidia_gpu %}
# GPU: NVIDIA CUDA enabled
{% endif %}

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
PURPLE='\033[0;35m'
NC='\033[0m'

# Configuration
OASENTINEL_HOME="{{ oasentinel_dir }}"
CONFIG_FILE="{{ oasentinel.config_file | default('configs/ubuntu_gpu.yaml') }}"
MODEL_ARCH="{{ oasentinel.model_arch | default('yolo11m') }}"
EPOCHS="{{ oasentinel.epochs | default('100') }}"
BATCH_SIZE="{{ oasentinel.batch_size | default('16') }}"
{% if ml_has_nvidia_gpu is defined and ml_has_nvidia_gpu %}
DEVICE="{{ oasentinel.device | default('0') }}"
{% else %}
DEVICE="cpu"
{% endif %}

# Training session configuration
SESSION_NAME="oasentinel-training"
LOG_DIR="$OASENTINEL_HOME/logs/training"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="$LOG_DIR/training_${TIMESTAMP}.log"

# Logging functions
log_info() { 
    echo -e "${BLUE}[$(date +'%H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}
log_success() { 
    echo -e "${GREEN}[$(date +'%H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}
log_warning() { 
    echo -e "${YELLOW}[$(date +'%H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}
log_error() { 
    echo -e "${RED}[$(date +'%H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}
log_header() { 
    echo -e "\n${CYAN}===== $(date +'%H:%M:%S') - $1 =====${NC}" | tee -a "$LOG_FILE"
}

show_usage() {
    echo "oaSentinel Automated Training Script"
    echo ""
    echo "Usage: $0 [options]"
    echo ""
    echo "Options:"
    echo "  --config FILE      Training configuration file [default: $CONFIG_FILE]"
    echo "  --model ARCH       Model architecture (yolo11n/s/m/l/x) [default: $MODEL_ARCH]"
    echo "  --epochs NUM       Number of training epochs [default: $EPOCHS]"
    echo "  --batch SIZE       Batch size [default: $BATCH_SIZE]"
    echo "  --device DEVICE    Training device [default: $DEVICE]"
    echo "  --wandb           Enable Weights & Biases tracking"
    echo "  --resume PATH     Resume from checkpoint"
    echo "  --no-setup        Skip environment setup"
    echo "  --continuous      Continuous training (restart after completion)"
    echo "  -h, --help        Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0                           # Train with defaults"
    echo "  $0 --model yolo11s --epochs 50  # Quick training"
    echo "  $0 --wandb --continuous      # Tracked continuous training"
    echo ""
}

# Parse arguments
ENABLE_WANDB=false
RESUME_PATH=""
SKIP_SETUP=false  
CONTINUOUS_TRAINING=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --config)
            CONFIG_FILE="$2"
            shift 2
            ;;
        --model)
            MODEL_ARCH="$2"
            shift 2
            ;;
        --epochs)
            EPOCHS="$2"
            shift 2
            ;;
        --batch)
            BATCH_SIZE="$2"
            shift 2
            ;;
        --device)
            DEVICE="$2"
            shift 2
            ;;
        --wandb)
            ENABLE_WANDB=true
            shift
            ;;
        --resume)
            RESUME_PATH="$2"
            shift 2
            ;;
        --no-setup)
            SKIP_SETUP=true
            shift
            ;;
        --continuous)
            CONTINUOUS_TRAINING=true
            shift
            ;;
        -h|--help)
            show_usage
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            show_usage
            exit 1
            ;;
    esac
done

# Navigate to project directory
cd "$OASENTINEL_HOME"

# Create log directory
mkdir -p "$LOG_DIR"

# Start logging
log_header "oaSentinel Automated Training Started"
log_info "Host: {{ inventory_hostname }}"
log_info "Configuration: $CONFIG_FILE"
log_info "Model: $MODEL_ARCH"
log_info "Epochs: $EPOCHS"
log_info "Batch Size: $BATCH_SIZE"
log_info "Device: $DEVICE"
log_info "Log File: $LOG_FILE"

# Verify virtual environment
if [ ! -f ".venv/bin/activate" ]; then
    log_error "Virtual environment not found"
    log_info "Run: oas-setup to initialize environment"
    exit 1
fi

# Activate virtual environment
log_info "Activating virtual environment..."
source .venv/bin/activate

# Environment setup (unless skipped)
if [ "$SKIP_SETUP" = false ]; then
    log_header "Environment Setup"
    
    # Run setup script if available
    if [ -f "setup_training.sh" ]; then
        log_info "Running environment setup..."
        ./setup_training.sh --verify-only || {
            log_warning "Setup verification failed, running full setup..."
            ./setup_training.sh --no-data || {
                log_error "Environment setup failed"
                exit 1
            }
        }
    fi
    
    # Verify GPU status
{% if ml_has_nvidia_gpu is defined and ml_has_nvidia_gpu %}
    log_info "Checking GPU status..."
    if command -v nvidia-smi >/dev/null 2>&1; then
        nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv | tee -a "$LOG_FILE"
        
        # Check CUDA in PyTorch
        python3 -c "
import torch
print(f'PyTorch CUDA: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU Count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
        print(f'  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB')
" | tee -a "$LOG_FILE"
    else
        log_warning "nvidia-smi not available"
    fi
{% endif %}
fi

# Dataset verification
log_header "Dataset Verification"
if [ ! -f "crowdhuman.yaml" ]; then
    log_warning "Dataset configuration not found: crowdhuman.yaml"
    log_info "Attempting to use default dataset configuration..."
    
    # Try to find dataset config in common locations
    for config_candidate in "data/crowdhuman.yaml" "configs/crowdhuman.yaml" "data/processed/crowdhuman/dataset.yaml"; do
        if [ -f "$config_candidate" ]; then
            log_info "Using dataset config: $config_candidate"
            ln -sf "$config_candidate" crowdhuman.yaml
            break
        fi
    done
    
    if [ ! -f "crowdhuman.yaml" ]; then
        log_error "No dataset configuration found"
        log_info "Please ensure dataset is properly prepared"
        exit 1
    fi
fi

# Check dataset files
python3 -c "
import yaml
import os

try:
    with open('crowdhuman.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    train_path = config.get('train', '')
    val_path = config.get('val', '')
    
    if train_path and os.path.exists(train_path):
        train_count = len([f for f in os.listdir(train_path) if f.endswith(('.jpg', '.jpeg', '.png'))])
        print(f'[OK] Training images: {train_count}')
    else:
        print(f'[FAIL] Training path not found or empty: {train_path}')
        exit(1)
    
    if val_path and os.path.exists(val_path):
        val_count = len([f for f in os.listdir(val_path) if f.endswith(('.jpg', '.jpeg', '.png'))])
        print(f'[OK] Validation images: {val_count}')
    else:
        print(f'[WARNING] Validation path not found: {val_path}')
    
    print('Dataset verification passed')
    
except Exception as e:
    print(f'Dataset verification failed: {e}')
    exit(1)
" | tee -a "$LOG_FILE" || {
    log_error "Dataset verification failed"
    exit 1
}

# Training function
train_model() {
    local run_number=$1
    local run_suffix=""
    
    if [ "$CONTINUOUS_TRAINING" = true ] && [ "$run_number" -gt 1 ]; then
        run_suffix="_run${run_number}"
    fi
    
    log_header "Training Session ${run_number}"
    
    # Prepare training arguments
    local train_args=(
        "--config" "$CONFIG_FILE"
        "--model" "$MODEL_ARCH"
        "--epochs" "$EPOCHS"
        "--device" "$DEVICE"
    )
    
    # Add optional arguments
    if [ "$ENABLE_WANDB" = true ]; then
        train_args+=("--wandb")
        export WANDB_MODE="online"
        export WANDB_NAME="ubuntu_${MODEL_ARCH}_${TIMESTAMP}${run_suffix}"
    else
        export WANDB_MODE="disabled"
    fi
    
    if [ -n "$RESUME_PATH" ] && [ -f "$RESUME_PATH" ]; then
        train_args+=("--resume" "$RESUME_PATH")
        log_info "Resuming from: $RESUME_PATH"
    fi
    
    # Start training
    log_info "Starting training with arguments: ${train_args[*]}"
    log_info "Training output will be logged to: $LOG_FILE"
    
    # Run training script
    if [ -f "scripts/train.sh" ]; then
        ./scripts/train.sh "${train_args[@]}" 2>&1 | tee -a "$LOG_FILE"
        local train_result=${PIPESTATUS[0]}
    else
        log_error "Training script not found: scripts/train.sh"
        return 1
    fi
    
    if [ $train_result -eq 0 ]; then
        log_success "Training session $run_number completed successfully!"
        
        # Find the latest model
        local latest_model=$(find models/checkpoints -name "weights" -type d | head -1)
        if [ -n "$latest_model" ] && [ -f "$latest_model/best.pt" ]; then
            log_info "Best model saved: $latest_model/best.pt"
            
            # Set for potential resume in continuous training
            RESUME_PATH="$latest_model/last.pt"
            
            # Auto-evaluation if requested
            if [ -f "scripts/evaluate.sh" ] && [ "{{ oasentinel.auto_evaluate | default(true) }}" = "true" ]; then
                log_info "Running automatic evaluation..."
                ./scripts/evaluate.sh --model "$latest_model/best.pt" 2>&1 | tee -a "$LOG_FILE" || {
                    log_warning "Evaluation failed, continuing..."
                }
            fi
            
        fi
        
        return 0
    else
        log_error "Training session $run_number failed with exit code: $train_result"
        return $train_result
    fi
}

# Main training loop
log_header "Starting Training"

run_number=1
while true; do
    # System status before training
    log_info "System status before training run $run_number:"
    echo "Memory usage: $(free -h | grep '^Mem:' | awk '{print $3 "/" $2}')" | tee -a "$LOG_FILE"
    echo "Disk usage: $(df -h / | tail -1 | awk '{print $3 "/" $2 " (" $5 ")"}')" | tee -a "$LOG_FILE"
{% if ml_has_nvidia_gpu is defined and ml_has_nvidia_gpu %}
    if command -v nvidia-smi >/dev/null 2>&1; then
        echo "GPU status:" | tee -a "$LOG_FILE"
        nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits | tee -a "$LOG_FILE"
    fi
{% endif %}
    
    # Run training
    if train_model $run_number; then
        log_success "Training run $run_number completed successfully"
        
        if [ "$CONTINUOUS_TRAINING" = true ]; then
            log_info "Continuous training enabled, starting next run in 60 seconds..."
            sleep 60
            run_number=$((run_number + 1))
            
            # Check if we should continue (basic resource checks)
            if [ $(df / | tail -1 | awk '{print $5}' | sed 's/%//') -gt 90 ]; then
                log_warning "Disk usage >90%, stopping continuous training"
                break
            fi
            
            continue
        else
            break
        fi
    else
        log_error "Training run $run_number failed"
        
        if [ "$CONTINUOUS_TRAINING" = true ] && [ $run_number -lt 3 ]; then
            log_info "Retrying in continuous mode... (attempt $((run_number + 1)))"
            sleep 120
            run_number=$((run_number + 1))
            continue
        else
            exit 1
        fi
    fi
done

# Final summary
log_header "Training Summary"
log_success "oaSentinel training completed on {{ inventory_hostname }}"
log_info "Total training runs: $run_number"
log_info "Final log file: $LOG_FILE"

# Show final model location
if [ -n "$RESUME_PATH" ] && [ -f "$RESUME_PATH" ]; then
    log_info "Latest model: $RESUME_PATH"
    log_info "Model directory: $(dirname "$RESUME_PATH")"
fi

log_info ""
log_info "Next Steps:"
log_info "  oas-eval                     # Evaluate the trained model"
log_info "  oas-export                   # Export for deployment"
log_info "  oas-logs                     # View training logs"
log_info ""
log_success "Training automation complete!"