---
# ML Local Development Server Inventory
# For setting up oaSentinel training on local VM (192.168.2.250)

all:
  children:
    ubuntu:
      hosts:
        kai-vm-u1:
          ansible_host: 192.168.2.250
          ansible_user: kai
          ansible_become_password: "{{ vault_sudo_passwords['kai-vm-u1'] }}"

          # Hardware specifications (adjust based on your VM specs)
          gpu_count: 1
          gpu_type: "GPU (Local VM)"
          total_memory_gb: 16  # Adjust based on your VM allocation
          available_storage_gb: 100  # Adjust based on your VM disk

      vars:
        # Platform identification
        ansible_distribution: Ubuntu
        ansible_distribution_version: "24.04"

        # Component-based deployment configuration
        selected_components:
          - base
          - network
          - security
          - docker
          - nvidia
          - ml
          - monitoring
          - health
          - oasentinel-full

        # Server configuration
        is_gpu_server: true
        enable_remote_access: true
        setup_ml_environment: true # Enable ML workstation setup

        # Role execution control
        execute_base_setup: true
        execute_nvidia_setup: true
        execute_docker_setup: true
        execute_ml_setup: true
        execute_monitoring_setup: true

        # oaSentinel specific settings
        oasentinel_repo_branch: main
        python_version: "3.11" # We'll use 3.11 for compatibility with existing oaSentinel

        # CUDA 12.8 Configuration - Pure focus on CUDA toolkit
        cuda_version: "12.8"
        cuda_install_path: "/usr/local/cuda-12.8"
        cuda_symlink_path: "/usr/local/cuda"
        ignore_missing_gpu: true  # Temporarily bypass hardware check for PATH fix

        # Ubuntu ML optimization for local development
        ubuntu_ml_optimization:
          increase_shared_memory: true
          optimize_swap: true
          increase_file_limits: true
          install_build_essentials: true
          enable_performance_governor: true

        # Training server features
        training_server:
          enable_jupyter: true
          jupyter_port: 8888
          enable_tensorboard: true
          tensorboard_port: 6006
          setup_screen_sessions: true

        # Data management optimized for local development
        data_management:
          create_datasets_dir: true
          datasets_path: "/data/ml-datasets"
          optimize_io: true

        # Docker ML configuration with GPU support
        docker_ml_config:
          install_nvidia_runtime: true
          enable_gpu_support: true

        # Training configuration optimized for local development
        training_config:
          default_epochs: 50  # Reduced for local dev (vs 100 for remote)
          default_batch_size: 16 # Reduced for local VM (vs 32 for remote)
          auto_device_detection: true
          mixed_precision: true

        # ML frameworks with GPU focus
        ml_frameworks:
          pytorch: true
          ultralytics: true
          opencv: true
          jupyter: true
          wandb: true # Enable for professional training tracking

        # Performance optimization for local development
        performance_optimization:
          enable_gpu_monitoring: true
          optimize_memory: true
          enable_profiling: true

        # Remote access settings for local development
        remote_access:
          monitoring_dashboard: true
          setup_reverse_tunnel: false

        # Development tools enabled
        dev_tools:
          enable_jupyter: true
          enable_tensorboard: true
          enable_code_formatting: true
          enable_testing: true
