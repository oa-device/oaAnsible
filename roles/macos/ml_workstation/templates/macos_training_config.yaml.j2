# macOS Optimized Training Configuration for oaSentinel
# Generated by Ansible oaAnsible/roles/macos/ml_workstation
# Optimized for {{ 'Apple Silicon' if is_apple_silicon else 'Intel' }} architecture

# Model configuration
model_name: "yolo11n.pt"
model_size: "n"  # Start with nano for faster training on macOS
input_size: 640

# Dataset configuration
dataset_name: "crowdhuman"
num_classes: 1
class_names: ["person"]

# Training parameters optimized for macOS
epochs: 50  # Reduced for faster iteration on local machine
{% if ansible_memtotal_mb|int >= 16384 %}
batch_size: 16  # Higher batch size for machines with 16GB+ RAM
{% else %}
batch_size: 8   # Conservative batch size for machines with less RAM
{% endif %}
learning_rate: 0.001
patience: 15
save_period: 10

# Hardware-specific optimizations
{% if is_apple_silicon %}
device: "mps"  # Use Metal Performance Shaders on Apple Silicon
{% else %}
device: "cpu"  # Use CPU on Intel Macs (or "auto" for GPU detection)
{% endif %}
workers: {{ [ansible_processor_vcpus|int, 8]|min }}  # Limit workers to avoid overloading

# macOS-specific optimization settings
optimizer: "AdamW"  # Generally performs better on Apple Silicon
lr_scheduler: "cosine"
warmup_epochs: 3
weight_decay: 0.0005
momentum: 0.937

# Data augmentation (conservative for local development)
augment: true
mixup: false  # Disabled for faster training
copy_paste: false  # Disabled for faster training

# Output settings
project_name: "oaSentinel-macOS"
experiment_name: "{{ 'apple-silicon' if is_apple_silicon else 'intel' }}-baseline"

# Validation settings (adjusted for development)
val_split: 0.2
test_split: 0.0  # Skip test split for faster development cycles

# Performance settings
half_precision: {{ is_apple_silicon }}  # Enable FP16 on Apple Silicon
optimize: true
freeze: null  # Don't freeze layers initially

# Export settings for macOS deployment
export_formats: ["coreml", "onnx"]

# Logging and monitoring
verbose: true
plots: true
save_json: true
save_hybrid: false

# macOS-specific notes
# - Optimized for development and testing workflows
# - Use this config for initial training and experimentation
# - For production training, consider using a GPU server
# - CoreML export enabled for deployment to Mac Minis
{% if is_apple_silicon %}
# - MPS backend provides significant acceleration on Apple Silicon
# - Monitor memory usage during training to avoid swapping
{% else %}
# - CPU training will be slower but still functional
# - Consider using a remote GPU server for production training
{% endif %}